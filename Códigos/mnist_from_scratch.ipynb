{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse notebook, será construído uma rede neural à mão que consegue reconhecer as imagens dos números do conjunto de dados [MNIST](https://en.wikipedia.org/wiki/MNIST_database). Foram utilizadas as mesmas classes e funções do notebook [XOR à mão](xor_from_scratch.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function(ABC):\n",
    "    \"\"\"\n",
    "    Classe abstrata para as funções de ativação e de loss.\n",
    "    Exige que suas subclasses possuam uma função f e a derivada dessa função f_prime\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def f(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def f_prime(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Function):\n",
    "    def f(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def f_prime(self, x):\n",
    "        return self.f(x) * (1-self.f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Function):\n",
    "    def f(self, x):\n",
    "        return (x > 0) * x\n",
    "    def f_prime(self, x):\n",
    "        return (x > 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Function):\n",
    "    def f(self, y_hat, y):\n",
    "        return np.sum((y_hat - y)**2)/len(y_hat)\n",
    "    def f_prime(self, y_hat, y):\n",
    "        return 2*(y_hat - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_of_inputs: int, n_of_neurons: int , activation: Function, bias: float=0.0):\n",
    "        self.n_of_inputs = n_of_inputs\n",
    "        self.n_of_neurons = n_of_neurons\n",
    "        self.activation = activation\n",
    "        self.bias = np.ones((1, n_of_neurons)) * bias # bias, inicializado como 0 por padrão\n",
    "        self.weights = np.random.uniform(-1, 1, (n_of_inputs, n_of_neurons)) # matriz de pesos \n",
    "        \n",
    "        # As variáveis abaixo são necessárias para o backward\n",
    "        self.weight_gradient = None  # vetor de gradiente dos pesos\n",
    "        self.bias_gradient = None # vetor de gradiente do bias\n",
    "        self.layer_inputs = None # output da camada anterior, ou as entradas da rede caso for a primeira camada\n",
    "        self.linear_output = None # resultado antes de ser aplicada a função de ativação -> linear_output = a @ w + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation da camada\n",
    "        \"\"\"\n",
    "        # Shapes:\n",
    "        # Primeira para a segunda camada: (1, 2) @ (2, 2) = (1, 2)\n",
    "        # Segunda para a terceira camada: (1, 2) @ (2, 1) = (1, 1)\n",
    "        self.layer_inputs = x \n",
    "        dot_product = self.layer_inputs @ self.weights \n",
    "        self.linear_output = dot_product + self.bias\n",
    "        output = self.activation.f(self.linear_output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, chain_rule_derivatives):\n",
    "        \"\"\"\n",
    "        Cálculo dos gradientes da camada. \n",
    "        É calculada as derivadas em relação a matriz de pesos e o bias da camada (dC_dw e dC_db), e a \n",
    "        derivada em relação ao linear_output (dC_da), para que possa mandar essa derivada para trás para calcular\n",
    "        o gradiente dos pesos das camadas anteriores, conforme o diagrama\n",
    "        Parâmetros:\n",
    "        chain_rule_derivatives - derivada calculada através da regra da cadeia, que foi mandada da camada seguinte (dC_da1)\n",
    "        Retorno:\n",
    "        chain_rule_derivatives - derivada calculada através da regra da cadeia, para ser mandada para a camada anterior (dc_da0)\n",
    "        \"\"\"\n",
    "        da1_dz = self.activation.f_prime(self.linear_output) \n",
    "        dz_dw = self.layer_inputs\n",
    "        dz_da0 = self.weights\n",
    "        \n",
    "        dC_dw = dz_dw.T @ (da1_dz * chain_rule_derivatives) \n",
    "        dC_db = 1 * da1_dz * chain_rule_derivatives\n",
    "        dC_da0 = (chain_rule_derivatives * da1_dz) @ dz_da0.T\n",
    "        \n",
    "        chain_rule_derivatives = dC_da0\n",
    "        self.weight_gradient = dC_dw\n",
    "        self.bias_gradient = dC_db\n",
    "        \n",
    "        return chain_rule_derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, lr):\n",
    "        self.layers = []\n",
    "        self.input_size = input_size\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation da rede\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, loss_derivative):\n",
    "        \"\"\"\n",
    "        Backward propagation da rede.\n",
    "        Calcula os gradientes e aplica o algoritmo de gradiente descendente para atualizar os pesos e os bias\n",
    "        \"\"\"\n",
    "        # Cálculo dos gradientes\n",
    "        chain_rule_derivatives = loss_derivative\n",
    "        for layer in reversed(self.layers):\n",
    "            chain_rule_derivatives = layer.backward(chain_rule_derivatives)\n",
    "        \n",
    "        # Gradiente descendente\n",
    "        for layer in self.layers:\n",
    "            layer.weights -= layer.weight_gradient * self.lr\n",
    "            layer.bias -= layer.bias_gradient * self.lr\n",
    "\n",
    "    # Faz o forward chamando o objeto, passando os inputs como parâmetro, da mesma forma que o PyTorch faz\n",
    "    def __call__(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "\n",
    "    def append_layer(self, output_number: int, activation, bias: float=0.0):\n",
    "        \"\"\"\n",
    "        Dado um número de saída adiciona uma camada ao fim da rede neural\n",
    "        Ex: nn = NeuralNetwork(...)\n",
    "          nn.append_layer(...)\n",
    "          nn.append_layer(...)\n",
    "          ...\n",
    "        \"\"\"\n",
    "        # Caso seja a primeira camada\n",
    "        if len(self.layers) == 0:\n",
    "            new_layer_input = self.input_size\n",
    "        else:\n",
    "            new_layer_input = self.layers[-1].n_of_neurons\n",
    "\n",
    "        self.layers.append(Layer(new_layer_input, output_number, activation, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria vetor one hot mnist\n",
    "def one_hot(value: int):\n",
    "    one_hot_vec = np.zeros((1, 10))\n",
    "    one_hot_vec[0][value] = 1\n",
    "    return one_hot_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set is 60000\n",
      "Size of test set is 10000\n",
      "[0, 20001] Accumulated Loss: 8.173645930216533e-05\n",
      "[0, 40001] Accumulated Loss: 7.590054531862076e-05\n",
      "Loss: 1.2482032712511677 - Epoch: 1\n",
      "\n",
      "Epoch Accuracy: 71.85000000000001%\n",
      "\n",
      "[1, 20001] Accumulated Loss: 1.9677267203508734e-05\n",
      "[1, 40001] Accumulated Loss: 1.847541873891639e-05\n",
      "Loss: 0.3444905154554732 - Epoch: 2\n",
      "\n",
      "Epoch Accuracy: 83.93%\n",
      "\n",
      "[2, 20001] Accumulated Loss: 1.373712253907718e-05\n",
      "[2, 40001] Accumulated Loss: 1.3424061316576966e-05\n",
      "Loss: 0.2567657099762351 - Epoch: 3\n",
      "\n",
      "Epoch Accuracy: 86.61999999999999%\n",
      "\n",
      "[3, 20001] Accumulated Loss: 1.1251232598180684e-05\n",
      "[3, 40001] Accumulated Loss: 1.1181469221829684e-05\n",
      "Loss: 0.21610109058853308 - Epoch: 4\n",
      "\n",
      "Epoch Accuracy: 88.53999999999999%\n",
      "\n",
      "[4, 20001] Accumulated Loss: 9.831764285701646e-06\n",
      "[4, 40001] Accumulated Loss: 9.862222120517024e-06\n",
      "Loss: 0.19167733233027642 - Epoch: 5\n",
      "\n",
      "Epoch Accuracy: 89.4%\n",
      "\n",
      "[5, 20001] Accumulated Loss: 8.888930773724612e-06\n",
      "[5, 40001] Accumulated Loss: 8.968867443150483e-06\n",
      "Loss: 0.17493153841046027 - Epoch: 6\n",
      "\n",
      "Epoch Accuracy: 90.2%\n",
      "\n",
      "[6, 20001] Accumulated Loss: 8.204326206554556e-06\n",
      "[6, 40001] Accumulated Loss: 8.310400490108872e-06\n",
      "Loss: 0.16247892572250683 - Epoch: 7\n",
      "\n",
      "Epoch Accuracy: 90.68%\n",
      "\n",
      "[7, 20001] Accumulated Loss: 7.674588001752823e-06\n",
      "[7, 40001] Accumulated Loss: 7.795219289246733e-06\n",
      "Loss: 0.1526752129575945 - Epoch: 8\n",
      "\n",
      "Epoch Accuracy: 90.96%\n",
      "\n",
      "[8, 20001] Accumulated Loss: 7.245041825723982e-06\n",
      "[8, 40001] Accumulated Loss: 7.374003406012672e-06\n",
      "Loss: 0.1446264733508089 - Epoch: 9\n",
      "\n",
      "Epoch Accuracy: 91.44%\n",
      "\n",
      "[9, 20001] Accumulated Loss: 6.885256409656924e-06\n",
      "[9, 40001] Accumulated Loss: 7.018842020267834e-06\n",
      "Loss: 0.13781868866865693 - Epoch: 10\n",
      "\n",
      "Epoch Accuracy: 91.74%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading MNIST dataset\n",
    "data_path='/data/mnist'\n",
    "mnist_train = MNIST(data_path, train=True, transform=transforms.ToTensor(),)\n",
    "mnist_test = MNIST(data_path, train=False, transform=transforms.ToTensor(),)\n",
    "num_classes = 10  # MNIST has 10 output classes\n",
    "train_set = mnist_train.data.numpy()/255\n",
    "train_targets = [one_hot(t) for t in mnist_train.targets]\n",
    "test_set = mnist_test.data.numpy()/255\n",
    "test_targets = [one_hot(t) for t in mnist_test.targets]\n",
    "print(f\"Size of train set is {len(train_set)}\")\n",
    "print(f\"Size of test set is {len(test_set)}\")\n",
    "lr = 0.003\n",
    "nn = NeuralNetwork(28*28, lr)\n",
    "nn.append_layer(64, activation=Sigmoid(), bias=1)\n",
    "nn.append_layer(64, activation=Sigmoid(), bias=1)\n",
    "nn.append_layer(10, activation=Sigmoid(), bias=1)\n",
    "criterion = MSELoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for i, (x, y) in enumerate(zip(train_set, train_targets)):\n",
    "        y_hat = nn(x.flatten().reshape(1,-1))\n",
    "        loss = criterion.f(y_hat, y)\n",
    "        loss_derivative = criterion.f_prime(y_hat, y)\n",
    "        nn.backward(loss_derivative)\n",
    "        total_loss += loss\n",
    "        if i % 20000 == 0 and i > 0:\n",
    "            print(f\"[{epoch}, {i+1:5d}] Accumulated Loss: {total_loss/(20000 * i)}\")\n",
    "\n",
    "    print(f\"Loss: {total_loss / len(train_set)} - Epoch: {epoch + 1}\")\n",
    "\n",
    "    # validate\n",
    "    hits = 0\n",
    "    for x, y in zip(test_set, test_targets):\n",
    "        y_hat = nn(x.flatten().reshape(1, -1))\n",
    "        if np.argmax(y_hat) == np.argmax(y):\n",
    "            hits += 1\n",
    "    print(f'\\nEpoch Accuracy: {hits/len(test_set)* 100}%\\n')\n",
    "\n",
    "\n",
    "def guess():\n",
    "    n = np.random.randint(0, len(test_set))\n",
    "    predicted = nn.forward(test_set[n].flatten().reshape(1, -1))\n",
    "    actual = test_targets[n]\n",
    "    print(f\"Actual number: {np.argmax(actual)} - Predicted number: {np.argmax(predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual number: 5 - Predicted number: 3\n"
     ]
    }
   ],
   "source": [
    "guess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
