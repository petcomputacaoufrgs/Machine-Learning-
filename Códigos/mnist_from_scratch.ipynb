{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse notebook, será construído uma rede neural à mão que consegue reconhecer as imagens dos números do conjunto de dados [MNIST](https://en.wikipedia.org/wiki/MNIST_database). Foram utilizadas as mesmas classes e funções do notebook [XOR à mão](xor_from_scratch.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def f(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def f_prime(self, x):\n",
    "        return self.f(x) * (1-self.f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def f(self, y_hat, y):\n",
    "        return np.sum((y_hat - y)**2)/len(y_hat)\n",
    "    def f_prime(self, y_hat, y):\n",
    "        return 2*(y_hat - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_of_inputs: int, n_of_neurons: int , activation, bias: float=0.0):\n",
    "        self.n_of_inputs = n_of_inputs\n",
    "        self.n_of_neurons = n_of_neurons\n",
    "        self.activation = activation\n",
    "        self.bias = np.ones((1, n_of_neurons)) * bias \n",
    "        self.weights = np.random.uniform(-1, 1, (n_of_inputs, n_of_neurons)) \n",
    "        \n",
    "        # As variáveis abaixo são necessárias para o backward\n",
    "        self.weight_gradient = None  \n",
    "        self.bias_gradient = None \n",
    "        self.layer_inputs = None # output da camada anterior, ou as entradas da rede caso for a primeira camada\n",
    "        self.linear_output = None # resultado antes de ser aplicada a função de ativação -> linear_output = a @ w + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation da camada\n",
    "        \"\"\"\n",
    "        self.layer_inputs = x \n",
    "        dot_product = self.layer_inputs @ self.weights \n",
    "        self.linear_output = dot_product + self.bias\n",
    "        output = self.activation.f(self.linear_output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, chain_rule_derivatives):\n",
    "        \"\"\"\n",
    "        Cálculo dos gradientes da camada. \n",
    "        É calculada as derivadas em relação a matriz de pesos e o bias da camada (dC_dw e dC_db), e a \n",
    "        derivada em relação ao linear_output (dC_da), para que possa mandar essa derivada para trás para calcular\n",
    "        o gradiente dos pesos das camadas anteriores, conforme o diagrama\n",
    "        Parâmetros:\n",
    "        chain_rule_derivatives - derivada calculada através da regra da cadeia, que foi mandada da camada seguinte (dC_da1)\n",
    "        Retorno:\n",
    "        updated_chain_rule_derivatives - derivada calculada através da regra da cadeia, para ser mandada para a camada anterior (dc_da0)\n",
    "        \"\"\"\n",
    "        da1_dz = self.activation.f_prime(self.linear_output) \n",
    "        dz_dw = self.layer_inputs\n",
    "        dz_da0 = self.weights\n",
    "        \n",
    "        dC_dw = dz_dw.T @ (da1_dz * chain_rule_derivatives) \n",
    "        dC_db = 1 * da1_dz * chain_rule_derivatives\n",
    "        dC_da0 = (chain_rule_derivatives * da1_dz) @ dz_da0.T\n",
    "        \n",
    "        updated_chain_rule_derivatives = dC_da0\n",
    "        self.weight_gradient = dC_dw\n",
    "        self.bias_gradient = dC_db\n",
    "        \n",
    "        return updated_chain_rule_derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, lr):\n",
    "        self.layers = []\n",
    "        self.input_size = input_size\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation da rede\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, loss_derivative):\n",
    "        \"\"\"\n",
    "        Backward propagation da rede.\n",
    "        Calcula os gradientes e aplica o algoritmo de gradiente descendente para atualizar os pesos e os bias\n",
    "        \"\"\"\n",
    "        # Cálculo dos gradientes\n",
    "        chain_rule_derivatives = loss_derivative\n",
    "        for layer in reversed(self.layers):\n",
    "            chain_rule_derivatives = layer.backward(chain_rule_derivatives)\n",
    "        \n",
    "        # Gradiente descendente\n",
    "        for layer in self.layers:\n",
    "            layer.weights -= layer.weight_gradient * self.lr\n",
    "            layer.bias -= layer.bias_gradient * self.lr\n",
    "\n",
    "    # Faz o forward chamando o objeto, passando os inputs como parâmetro, da mesma forma que o PyTorch faz\n",
    "    def __call__(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "\n",
    "    def append_layer(self, output_number: int, activation, bias: float=0.0):\n",
    "        \"\"\"\n",
    "        Dado um número de saída adiciona uma camada ao fim da rede neural\n",
    "        Ex: nn = NeuralNetwork(...)\n",
    "          nn.append_layer(...)\n",
    "          nn.append_layer(...)\n",
    "          ...\n",
    "        \"\"\"\n",
    "        # Caso seja a primeira camada\n",
    "        if len(self.layers) == 0:\n",
    "            new_layer_input = self.input_size\n",
    "        else:\n",
    "            new_layer_input = self.layers[-1].n_of_neurons\n",
    "\n",
    "        self.layers.append(Layer(new_layer_input, output_number, activation, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset MNIST\n",
    "Esse dataset é dividido em dois conjuntos nomeados **train** e **test**, em que o primeiro possui 60000 imagens e o segundo 10000.<br>\n",
    "Cada conjunto é dividido em **data**, que contêm as matrizes de pixeis da imagem, e **targets**, que contêm os números respectivos às matrizes de pixeis.\n",
    "Visualizações de como a rede neural funciona para esse problema podem ser encontradas no [MNIST PyTorch](mnist_pytorch.ipynb)\n",
    "## One hot encoding\n",
    "A saída da rede neural está na forma *one hot*, ou seja, um vetor com 10 posições, em que cada index é respectivo ao número da probabilidade gerada. O dataset MNIST vem com os targets em forma de número, sendo assim, nessa implementação é necessário converte-los para one hot.\n",
    "### Exemplos:\n",
    "**9** -> [0 0 0 0 0 0 0 0 0 1]<br>\n",
    "**4** -> [0 0 0 0 1 0 0 0 0 0]<br>\n",
    "**7** -> [0 0 0 0 0 0 0 1 0 0]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(value: int):\n",
    "    one_hot_vec = np.zeros((1, 10))\n",
    "    one_hot_vec[0][value] = 1\n",
    "    return one_hot_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vitor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "mnist_train = MNIST(root='./data', train=True, download=False)\n",
    "mnist_test = MNIST(root='./data', train=False, download=False) \n",
    "\n",
    "train_data = np.array(mnist_train.data)\n",
    "train_targets = np.array([one_hot(t.item()) for t in mnist_train.targets])\n",
    "\n",
    "test_data = np.array(mnist_test.data)\n",
    "test_targets = np.array([one_hot(t.item()) for t in mnist_test.targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-53f0d3a21cd6>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6638958228340215 - Epoch: 1\n",
      "\n",
      "Epoch Accuracy: 67.71000000000001%\n",
      "\n",
      "Loss: 0.43152392365675235 - Epoch: 2\n",
      "\n",
      "Epoch Accuracy: 76.75%\n",
      "\n",
      "Loss: 0.36566674765549695 - Epoch: 3\n",
      "\n",
      "Epoch Accuracy: 78.97999999999999%\n",
      "\n",
      "Loss: 0.3224660885218194 - Epoch: 4\n",
      "\n",
      "Epoch Accuracy: 82.28999999999999%\n",
      "\n",
      "Loss: 0.2954003052815908 - Epoch: 5\n",
      "\n",
      "Epoch Accuracy: 82.37%\n",
      "\n",
      "Loss: 0.2924789001781059 - Epoch: 6\n",
      "\n",
      "Epoch Accuracy: 83.03%\n",
      "\n",
      "Loss: 0.26539080887528044 - Epoch: 7\n",
      "\n",
      "Epoch Accuracy: 85.22%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.003\n",
    "criterion = MSELoss()\n",
    "model = NeuralNetwork(28*28, lr)\n",
    "model.append_layer(64, activation=Sigmoid(), bias=1)\n",
    "model.append_layer(64, activation=Sigmoid(), bias=1)\n",
    "model.append_layer(10, activation=Sigmoid(), bias=1)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for i, training_sample in enumerate(zip(train_data, train_targets)):\n",
    "        x = training_sample[0].reshape(1,-1)\n",
    "        y = training_sample[1]\n",
    "        y_hat = model(x)\n",
    "        loss = criterion.f(y_hat, y)\n",
    "        loss_derivative = criterion.f_prime(y_hat, y)\n",
    "        model.backward(loss_derivative)\n",
    "        total_loss += loss\n",
    "    print(f\"Loss: {total_loss / len(train_data)} - Epoch: {epoch + 1}\")\n",
    "\n",
    "    # Validar\n",
    "    hits = 0\n",
    "    for x, y in zip(test_data, test_targets):\n",
    "        y_hat = model(x.reshape(1, -1))\n",
    "        if np.argmax(y_hat) == np.argmax(y):\n",
    "            hits += 1\n",
    "    print(f'\\nEpoch Accuracy: {hits/len(test_data)* 100}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nota-se que o modelo obteve uma alta acurácia, considerando que treinou apenas por 10 épocas, utilizando uma função custo e um otimizador simples (MSELoss e Gradiente Descendente). No [MNIST PyTorch](mnist_pytorch.ipynb), o modelo treinado fica com uma acurácia melhor, devido ao leque de possibilidades que o PyTorch oferece para montar e treinar o modelo."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def guess():\n",
    "    # Gera um número aleatório no conjunto de dados de teste e o modelo tenta prever qual é esse número \n",
    "    n = np.random.randint(0, len(test_data))\n",
    "    predicted = model.forward(test_data[n].reshape(1, -1))\n",
    "    actual = test_targets[n]\n",
    "    print(f\"Actual number: {np.argmax(actual)} - Predicted number: {np.argmax(predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "guess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}