{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse notebook, será construído uma rede neural a mão que consegue simular a porta lógica XOR, utilizando apenas a biblioteca NumPy. Enquanto os exemplos serão ilustrados considerando esse problema, essa mesma rede pode ser utilizado para resolver outros problemas mais complexos, como utilizada na [MNIST a mão](mnist_from_scratch.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Function(ABC):\n",
    "    \"\"\"\n",
    "    Classe abstrata para as funções de ativação e de loss.\n",
    "    Exige que suas subclasses possuam uma função f e a derivada dessa função f_prime\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def f(self, *args):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def f_prime(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Function):\n",
    "    def f(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def f_prime(self, x):\n",
    "        return self.f(x) * (1-self.f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Function):\n",
    "    def f(self, y_hat, y):\n",
    "        return np.sum((y_hat - y)**2)/len(y_hat)\n",
    "    def f_prime(self, y_hat, y):\n",
    "        return 2*(y_hat - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagrama da rede e derivadas\n",
    "A rede possuí duas entradas e uma saída correspondentes a tabela-verdade do XOR. Nesse caso ela também possuirá uma hidden layer com dois neurônios.\n",
    "![Xor diagram](Imagens/xor_diagram_nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, n_of_inputs: int, n_of_neurons: int , activation: Function, bias: float=0.0):\n",
    "        self.n_of_inputs = n_of_inputs\n",
    "        self.n_of_neurons = n_of_neurons\n",
    "        self.activation = activation\n",
    "        self.bias = np.ones((1, n_of_neurons)) * bias # bias, inicializado como 0 por padrão\n",
    "        self.weights = np.random.uniform(-1, 1, (n_of_inputs, n_of_neurons)) # matriz de pesos \n",
    "        \n",
    "        # As variáveis abaixo são necessárias para o backward\n",
    "        self.weight_gradient = None  # vetor de gradiente dos pesos\n",
    "        self.bias_gradient = None # vetor de gradiente do bias\n",
    "        self.layer_inputs = None # output da camada anterior, ou as entradas da rede caso for a primeira camada\n",
    "        self.linear_output = None # resultado antes de ser aplicada a função de ativação -> linear_output = a @ w + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation da camada\n",
    "        \"\"\"\n",
    "        # Shapes:\n",
    "        # Primeira para a segunda camada: (1, 2) @ (2, 2) = (1, 2)\n",
    "        # Segunda para a terceira camada: (1, 2) @ (2, 1) = (1, 1)\n",
    "        self.layer_inputs = x \n",
    "        dot_product = self.layer_inputs @ self.weights \n",
    "        self.linear_output = dot_product + self.bias\n",
    "        output = self.activation.f(self.linear_output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, chain_rule_derivatives):\n",
    "        \"\"\"\n",
    "        Cálculo dos gradientes da camada. \n",
    "        É calculada as derivadas em relação a matriz de pesos e o bias da camada (dC_dw e dC_db), e a \n",
    "        derivada em relação ao linear_output (dC_da), para que possa mandar essa derivada para trás para calcular\n",
    "        o gradiente dos pesos das camadas anteriores, conforme o diagrama\n",
    "        Parâmetros:\n",
    "        chain_rule_derivatives - derivada calculada através da regra da cadeia, que foi mandada da camada seguinte (dC_da1)\n",
    "        Retorno:\n",
    "        chain_rule_derivatives - derivada calculada através da regra da cadeia, para ser mandada para a camada anterior (dc_da0)\n",
    "        \"\"\"\n",
    "        da1_dz = self.activation.f_prime(self.linear_output) \n",
    "        dz_dw = self.layer_inputs\n",
    "        dz_da0 = self.weights\n",
    "        \n",
    "        dC_dw = dz_dw.T @ (da1_dz * chain_rule_derivatives) \n",
    "        dC_db = 1 * da1_dz * chain_rule_derivatives\n",
    "        dC_da0 = (chain_rule_derivatives * da1_dz) @ dz_da0.T\n",
    "        \n",
    "        updated_chain_rule_derivatives = dC_da0\n",
    "        self.weight_gradient = dC_dw\n",
    "        self.bias_gradient = dC_db\n",
    "        \n",
    "        return updated_chain_rule_derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classe Rede Neural\n",
    "Essa classe é a que será utilizada para construir o modelo. Utilizando a função *append layers* é possível adicionar quantas camadas que quiser à rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, lr):\n",
    "        self.layers = []\n",
    "        self.input_size = input_size\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation da rede\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, loss_derivative):\n",
    "        \"\"\"\n",
    "        Backward propagation da rede.\n",
    "        Calcula os gradientes e aplica o algoritmo de gradiente descendente para atualizar os pesos e os bias\n",
    "        \"\"\"\n",
    "        # Cálculo dos gradientes\n",
    "        chain_rule_derivatives = loss_derivative\n",
    "        for layer in reversed(self.layers):\n",
    "            chain_rule_derivatives = layer.backward(chain_rule_derivatives)\n",
    "        \n",
    "        # Gradiente descendente\n",
    "        for layer in self.layers:\n",
    "            layer.weights -= layer.weight_gradient * self.lr\n",
    "            layer.bias -= layer.bias_gradient * self.lr\n",
    "\n",
    "    # Faz o forward chamando o objeto, passando os inputs como parâmetro, da mesma forma que o PyTorch faz\n",
    "    def __call__(self, inputs):\n",
    "        return self.forward(inputs)\n",
    "\n",
    "    def append_layer(self, output_number: int, activation, bias: float=0.0):\n",
    "        \"\"\"\n",
    "        Dado um número de saída, adiciona uma camada ao fim da rede neural\n",
    "        Ex: nn = NeuralNetwork(...)\n",
    "          nn.append_layer(...)\n",
    "          nn.append_layer(...)\n",
    "          ...\n",
    "        \"\"\"\n",
    "        # Caso seja a primeira camada\n",
    "        if len(self.layers) == 0:\n",
    "            new_layer_input = self.input_size\n",
    "        else:\n",
    "            new_layer_input = self.layers[-1].n_of_neurons\n",
    "\n",
    "        self.layers.append(Layer(new_layer_input, output_number, activation, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela-verdade do XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uma dimensão extra é adicionada a cada combinação para ficar no formato (1, 2),\n",
    "# a fim de fazer as multiplicações de matrizes no forward\n",
    "X = np.array([[[0,0]], [[1, 0]], [[1, 1]], [[0, 1]]])\n",
    "y = np.array([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os pesos são inicializados selecionando aleatoriamente em uma distribuição uniforme entre -1 e 1. Dessa forma, pode ser que seja necessário rodar mais de uma vez para obter uma loss perto de 0, caso aconteça de por chance ter uma inicialização que não favoreça o aprendizado do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0717\n"
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "criterion = MSELoss()\n",
    "losses = []\n",
    "nn = NeuralNetwork(2, lr)\n",
    "nn.append_layer(2, activation=Sigmoid())\n",
    "nn.append_layer(1, activation=Sigmoid())\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for x, y0 in zip(X, y):\n",
    "        # Calcula a previsão\n",
    "        predicted = nn.forward(x)\n",
    "        # Computa a loss (float) e a derivada da loss (vetor)\n",
    "        loss = criterion.f(predicted, y0)\n",
    "        loss_derivative = criterion.f_prime(predicted, y0)\n",
    "        # Faz o backward da rede\n",
    "        nn.backward(loss_derivative)\n",
    "        total_loss += loss\n",
    "    losses.append(total_loss)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "print(f\"Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs: np.array):\n",
    "    predicted = nn.forward(inputs)\n",
    "    print(f\"Entrada: {inputs[0][0]}, {inputs[0][1]}\\nXOR: {round(predicted[0][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada: 0, 0\n",
      "XOR: 0\n",
      "Entrada: 1, 0\n",
      "XOR: 1\n",
      "Entrada: 1, 1\n",
      "XOR: 0\n",
      "Entrada: 0, 1\n",
      "XOR: 1\n"
     ]
    }
   ],
   "source": [
    "for x in X:\n",
    "    predict(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
